{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e933ae56",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 588/588 [00:00<00:00, 19734.58 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Training Baseline Model ---\n",
      "trainable params: 1,769,472 || all params: 141,191,424 || trainable%: 1.2532\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='370' max='370' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [370/370 00:31, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.561200</td>\n",
       "      <td>2.207599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2.568100</td>\n",
       "      <td>2.170262</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>2.444000</td>\n",
       "      <td>2.160821</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>2.414000</td>\n",
       "      <td>2.157857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>2.155200</td>\n",
       "      <td>2.150337</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/louideval/miniconda3/envs/NLP_BART_env/lib/python3.10/site-packages/peft/utils/save_and_load.py:309: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n",
      "/home/louideval/miniconda3/envs/NLP_BART_env/lib/python3.10/site-packages/peft/utils/save_and_load.py:309: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n",
      "/home/louideval/miniconda3/envs/NLP_BART_env/lib/python3.10/site-packages/peft/utils/save_and_load.py:309: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n",
      "/home/louideval/miniconda3/envs/NLP_BART_env/lib/python3.10/site-packages/peft/utils/save_and_load.py:309: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n",
      "/home/louideval/miniconda3/envs/NLP_BART_env/lib/python3.10/site-packages/peft/utils/save_and_load.py:309: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n",
      "/home/louideval/miniconda3/envs/NLP_BART_env/lib/python3.10/site-packages/peft/utils/save_and_load.py:309: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Training Style-Controlled Model ---\n",
      "trainable params: 1,769,472 || all params: 141,191,424 || trainable%: 1.2532\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='735' max='735' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [735/735 00:53, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.634300</td>\n",
       "      <td>2.364498</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2.599900</td>\n",
       "      <td>2.351474</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>2.723200</td>\n",
       "      <td>2.293809</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>2.347600</td>\n",
       "      <td>2.283524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>2.530000</td>\n",
       "      <td>2.276651</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/louideval/miniconda3/envs/NLP_BART_env/lib/python3.10/site-packages/peft/utils/save_and_load.py:309: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n",
      "/home/louideval/miniconda3/envs/NLP_BART_env/lib/python3.10/site-packages/peft/utils/save_and_load.py:309: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n",
      "/home/louideval/miniconda3/envs/NLP_BART_env/lib/python3.10/site-packages/peft/utils/save_and_load.py:309: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n",
      "/home/louideval/miniconda3/envs/NLP_BART_env/lib/python3.10/site-packages/peft/utils/save_and_load.py:309: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n",
      "/home/louideval/miniconda3/envs/NLP_BART_env/lib/python3.10/site-packages/peft/utils/save_and_load.py:309: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n",
      "/home/louideval/miniconda3/envs/NLP_BART_env/lib/python3.10/site-packages/peft/utils/save_and_load.py:309: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Complete!\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, DataCollatorForSeq2Seq, Seq2SeqTrainingArguments, Seq2SeqTrainer, set_seed\n",
    "from peft import get_peft_model, LoraConfig, TaskType\n",
    "\n",
    "# --- CONFIGURATION ---\n",
    "SEED = 42 \n",
    "set_seed(SEED) # <-- Set global seed here!\n",
    "MODEL_ID = \"facebook/bart-base\" # Small, fast, good for summarization\n",
    "MAX_INPUT_LEN = 256\n",
    "MAX_TARGET_LEN = 64\n",
    "LR = 2e-4\n",
    "BATCH_SIZE = 4 # Adjust based on GPU VRAM\n",
    "EPOCHS = 5\n",
    "OUTPUT_DIR_BASE = \"../output/baseline_model\"\n",
    "OUTPUT_DIR_STYLE = \"../output/style_model\"\n",
    "\n",
    "# 1. Load Data\n",
    "data_files = {\n",
    "    \"train\": \"../data/processed/train.csv\",\n",
    "    \"validation\": \"../data/processed/dev.csv\"\n",
    "}\n",
    "dataset = load_dataset(\"csv\", data_files=data_files)\n",
    "\n",
    "# 2. Tokenizer Setup & Special Tokens\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID)\n",
    "special_tokens = [\"<neutral>\", \"<punchy>\"]\n",
    "tokenizer.add_tokens(special_tokens)\n",
    "\n",
    "# --- PREPROCESSING FUNCTIONS ---\n",
    "\n",
    "def preprocess_baseline(examples):\n",
    "    # Baseline: Input = Snippet, Target = Neutral Headline (or random)\n",
    "    inputs = examples['snippet']\n",
    "    targets = examples['headline'] # Using actual headline as the default target for baseline\n",
    "    \n",
    "    model_inputs = tokenizer(inputs, max_length=MAX_INPUT_LEN, truncation=True)\n",
    "    labels = tokenizer(targets, max_length=MAX_TARGET_LEN, truncation=True)\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "def preprocess_style(examples):\n",
    "    # Style Model: Input = <STYLE> + Snippet, Target = Specific Headline\n",
    "    inputs = []\n",
    "    targets = []\n",
    "    \n",
    "    styles = ['neutral', 'punchy']\n",
    "    style_tokens = {'neutral': '<neutral>', 'punchy': '<punchy>'}\n",
    "    \n",
    "    for i in range(len(examples['snippet'])):\n",
    "        for style in styles:\n",
    "            # Construct Input: \"<style> snippet\"\n",
    "            input_text = f\"{style_tokens[style]} {examples['snippet'][i]}\"\n",
    "            # Construct Target: corresponding headline\n",
    "            target_text = examples[f'{style}'][i]\n",
    "            \n",
    "            inputs.append(input_text)\n",
    "            targets.append(target_text)\n",
    "            \n",
    "    model_inputs = tokenizer(inputs, max_length=MAX_INPUT_LEN, truncation=True)\n",
    "    labels = tokenizer(targets, max_length=MAX_TARGET_LEN, truncation=True)\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "# 3. Create Processed Datasets\n",
    "tokenized_baseline = dataset.map(preprocess_baseline, batched=True)\n",
    "# Note: Style preprocessing expands dataset size (1 story -> 3 pairs), so we map carefully\n",
    "# For simplicity in this script, we map then flatten, or use a custom generator. \n",
    "# Here is a simplified approach re-loading for style to ensure 1-to-many mapping:\n",
    "def flatten_style_data(batch):\n",
    "    new_rows = {'input': [], 'target': []}\n",
    "    styles = {'neutral': '<neutral>', 'punchy': '<punchy>'}\n",
    "    for i, snippet in enumerate(batch['snippet']):\n",
    "        for style_name, token in styles.items():\n",
    "            new_rows['input'].append(f\"{token} {snippet}\")\n",
    "            new_rows['target'].append(batch[f'{style_name}'][i])\n",
    "    return new_rows\n",
    "\n",
    "style_dataset = dataset.map(flatten_style_data, batched=True, remove_columns=dataset['train'].column_names)\n",
    "tokenized_style = style_dataset.map(lambda x: {\n",
    "    'input_ids': tokenizer(x['input'], max_length=MAX_INPUT_LEN, truncation=True)['input_ids'],\n",
    "    'labels': tokenizer(x['target'], max_length=MAX_TARGET_LEN, truncation=True)['input_ids']\n",
    "}, batched=True)\n",
    "\n",
    "# 4. LoRA Setup Function\n",
    "def get_lora_model():\n",
    "    model = AutoModelForSeq2SeqLM.from_pretrained(MODEL_ID)\n",
    "    model.resize_token_embeddings(len(tokenizer)) # Resize for new style tokens\n",
    "    \n",
    "    peft_config = LoraConfig(\n",
    "        task_type=TaskType.SEQ_2_SEQ_LM, \n",
    "        inference_mode=False, \n",
    "        r=64, \n",
    "        lora_alpha=128, \n",
    "        lora_dropout=0.1\n",
    "    )\n",
    "    model = get_peft_model(model, peft_config)\n",
    "    model.print_trainable_parameters()\n",
    "    return model\n",
    "\n",
    "# 5. Train Baseline\n",
    "print(\"\\n--- Training Baseline Model ---\")\n",
    "model_baseline = get_lora_model()\n",
    "args_base = Seq2SeqTrainingArguments(\n",
    "    output_dir=OUTPUT_DIR_BASE,\n",
    "    learning_rate=LR,\n",
    "    num_train_epochs=EPOCHS,\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_steps=10,\n",
    "    seed=SEED\n",
    ")\n",
    "\n",
    "trainer_base = Seq2SeqTrainer(\n",
    "    model=model_baseline,\n",
    "    args=args_base,\n",
    "    train_dataset=tokenized_baseline[\"train\"],\n",
    "    eval_dataset=tokenized_baseline[\"validation\"],\n",
    "    data_collator=DataCollatorForSeq2Seq(tokenizer, model=model_baseline),\n",
    ")\n",
    "trainer_base.train()\n",
    "model_baseline.save_pretrained(OUTPUT_DIR_BASE)\n",
    "\n",
    "# 6. Train Style Model\n",
    "print(\"\\n--- Training Style-Controlled Model ---\")\n",
    "model_style = get_lora_model() # Re-initialize fresh model\n",
    "args_style = Seq2SeqTrainingArguments(\n",
    "    output_dir=OUTPUT_DIR_STYLE,\n",
    "    learning_rate=LR,\n",
    "    num_train_epochs=EPOCHS,\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_steps=10,\n",
    "    seed=SEED\n",
    ")\n",
    "\n",
    "trainer_style = Seq2SeqTrainer(\n",
    "    model=model_style,\n",
    "    args=args_style,\n",
    "    train_dataset=tokenized_style[\"train\"],\n",
    "    eval_dataset=tokenized_style[\"validation\"], # Note: In real research, ensure validation doesn't overlap\n",
    "    data_collator=DataCollatorForSeq2Seq(tokenizer, model=model_style),\n",
    ")\n",
    "trainer_style.train()\n",
    "model_style.save_pretrained(OUTPUT_DIR_STYLE)\n",
    "tokenizer.save_pretrained(OUTPUT_DIR_STYLE) # Save tokenizer with new tokens\n",
    "\n",
    "print(\"Training Complete!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLP_BART_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
