{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e933ae56",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/louideval/miniconda3/envs/NLP_BART_env/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Training Baseline Model ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of PegasusForConditionalGeneration were not initialized from the model checkpoint at google/pegasus-large and are newly initialized: ['model.decoder.embed_positions.weight', 'model.encoder.embed_positions.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 46,137,344 || all params: 616,936,448 || trainable%: 7.4785\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='185' max='185' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [185/185 01:18, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.860300</td>\n",
       "      <td>2.251956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2.322100</td>\n",
       "      <td>2.142944</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>2.027100</td>\n",
       "      <td>2.102365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>2.020600</td>\n",
       "      <td>2.063111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.958800</td>\n",
       "      <td>2.062903</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Training Style-Controlled Model ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of PegasusForConditionalGeneration were not initialized from the model checkpoint at google/pegasus-large and are newly initialized: ['model.decoder.embed_positions.weight', 'model.encoder.embed_positions.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 46,137,344 || all params: 616,936,448 || trainable%: 7.4785\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='370' max='370' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [370/370 03:56, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.546700</td>\n",
       "      <td>2.280951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2.360900</td>\n",
       "      <td>2.177410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.982700</td>\n",
       "      <td>2.118946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.900500</td>\n",
       "      <td>2.117260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>2.010600</td>\n",
       "      <td>2.129208</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Complete!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, DataCollatorForSeq2Seq, Seq2SeqTrainingArguments, Seq2SeqTrainer, set_seed\n",
    "from peft import get_peft_model, LoraConfig, TaskType\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "# --- CONFIGURATION ---\n",
    "SEED = 42 \n",
    "set_seed(SEED) # <-- Set global seed here!\n",
    "MODEL_ID = \"google/pegasus-large\"\n",
    "MAX_INPUT_LEN = 256\n",
    "MAX_TARGET_LEN = 64\n",
    "LR = 2e-4\n",
    "BATCH_SIZE = 8 # Adjust based on GPU VRAM\n",
    "EPOCHS = 5\n",
    "OUTPUT_DIR_BASE = \"../output/baseline_model_PEGASUS\"\n",
    "OUTPUT_DIR_STYLE = \"../output/style_model_PEGASUS\"\n",
    "\n",
    "# 1. Load Data\n",
    "data_files = {\n",
    "    \"train\": \"../data/processed/train.csv\",\n",
    "    \"validation\": \"../data/processed/dev.csv\"\n",
    "}\n",
    "dataset = load_dataset(\"csv\", data_files=data_files)\n",
    "\n",
    "# 2. Tokenizer Setup & Special Tokens\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID)\n",
    "special_tokens = [\"<neutral>\", \"<punchy>\"]\n",
    "tokenizer.add_tokens(special_tokens)\n",
    "\n",
    "# --- PREPROCESSING FUNCTIONS ---\n",
    "\n",
    "def preprocess_baseline(examples):\n",
    "    # Baseline: Input = Snippet, Target = Neutral Headline (or random)\n",
    "    inputs = examples['snippet']\n",
    "    targets = examples['headline'] # Using actual headline as the default target for baseline\n",
    "    \n",
    "    model_inputs = tokenizer(inputs, max_length=MAX_INPUT_LEN, truncation=True)\n",
    "    labels = tokenizer(targets, max_length=MAX_TARGET_LEN, truncation=True)\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "def preprocess_style(examples):\n",
    "    # Style Model: Input = <STYLE> + Snippet, Target = Specific Headline\n",
    "    inputs = []\n",
    "    targets = []\n",
    "    \n",
    "    styles = ['neutral', 'punchy']\n",
    "    style_tokens = {'neutral': '<neutral>', 'punchy': '<punchy>'}\n",
    "    \n",
    "    for i in range(len(examples['snippet'])):\n",
    "        for style in styles:\n",
    "            # Construct Input: \"<style> snippet\"\n",
    "            input_text = f\"{style_tokens[style]} {examples['snippet'][i]}\"\n",
    "            # Construct Target: corresponding headline\n",
    "            target_text = examples[f'{style}'][i]\n",
    "            \n",
    "            inputs.append(input_text)\n",
    "            targets.append(target_text)\n",
    "            \n",
    "    model_inputs = tokenizer(inputs, max_length=MAX_INPUT_LEN, truncation=True)\n",
    "    labels = tokenizer(targets, max_length=MAX_TARGET_LEN, truncation=True)\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "# 3. Create Processed Datasets\n",
    "tokenized_baseline = dataset.map(preprocess_baseline, batched=True)\n",
    "# Note: Style preprocessing expands dataset size (1 story -> 3 pairs), so we map carefully\n",
    "# For simplicity in this script, we map then flatten, or use a custom generator. \n",
    "# Here is a simplified approach re-loading for style to ensure 1-to-many mapping:\n",
    "def flatten_style_data(batch):\n",
    "    new_rows = {'input': [], 'target': []}\n",
    "    styles = {'neutral': '<neutral>', 'punchy': '<punchy>'}\n",
    "    for i, snippet in enumerate(batch['snippet']):\n",
    "        for style_name, token in styles.items():\n",
    "            new_rows['input'].append(f\"{token} {snippet}\")\n",
    "            new_rows['target'].append(batch[f'{style_name}'][i])\n",
    "    return new_rows\n",
    "\n",
    "style_dataset = dataset.map(flatten_style_data, batched=True, remove_columns=dataset['train'].column_names)\n",
    "tokenized_style = style_dataset.map(lambda x: {\n",
    "    'input_ids': tokenizer(x['input'], max_length=MAX_INPUT_LEN, truncation=True)['input_ids'],\n",
    "    'labels': tokenizer(x['target'], max_length=MAX_TARGET_LEN, truncation=True)['input_ids']\n",
    "}, batched=True)\n",
    "\n",
    "# 4. LoRA Setup Function\n",
    "def get_lora_model():\n",
    "    model = AutoModelForSeq2SeqLM.from_pretrained(MODEL_ID)\n",
    "    model.resize_token_embeddings(len(tokenizer)) # Resize for new style tokens\n",
    "    \n",
    "    peft_config = LoraConfig(\n",
    "        task_type=TaskType.SEQ_2_SEQ_LM, \n",
    "        inference_mode=False, \n",
    "        r=64, # Increased rank as recommended \n",
    "        lora_alpha=128, \n",
    "        lora_dropout=0.1, # Specify T5/Flan-T5 attention layers \n",
    "        target_modules=[\n",
    "            \"q_proj\", \n",
    "            \"v_proj\", \n",
    "            \"k_proj\", \n",
    "            \"out_proj\", \n",
    "            \"fc1\", \n",
    "            \"fc2\" # Adding FFN layers\n",
    "        ],)\n",
    "    model = get_peft_model(model, peft_config)\n",
    "    model.print_trainable_parameters()\n",
    "    return model\n",
    "\n",
    "# 5. Train Baseline\n",
    "print(\"\\n--- Training Baseline Model ---\")\n",
    "model_baseline = get_lora_model()\n",
    "args_base = Seq2SeqTrainingArguments(\n",
    "    output_dir=OUTPUT_DIR_BASE,\n",
    "    learning_rate=LR,\n",
    "    num_train_epochs=EPOCHS,\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_steps=10,\n",
    "    seed=SEED\n",
    ")\n",
    "\n",
    "trainer_base = Seq2SeqTrainer(\n",
    "    model=model_baseline,\n",
    "    args=args_base,\n",
    "    train_dataset=tokenized_baseline[\"train\"],\n",
    "    eval_dataset=tokenized_baseline[\"validation\"],\n",
    "    data_collator=DataCollatorForSeq2Seq(tokenizer, model=model_baseline),\n",
    ")\n",
    "trainer_base.train()\n",
    "model_baseline.save_pretrained(OUTPUT_DIR_BASE)\n",
    "\n",
    "# 6. Train Style Model\n",
    "print(\"\\n--- Training Style-Controlled Model ---\")\n",
    "model_style = get_lora_model() # Re-initialize fresh model\n",
    "args_style = Seq2SeqTrainingArguments(\n",
    "    output_dir=OUTPUT_DIR_STYLE,\n",
    "    learning_rate=LR,\n",
    "    num_train_epochs=EPOCHS,\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_steps=10,\n",
    "    seed=SEED\n",
    ")\n",
    "\n",
    "trainer_style = Seq2SeqTrainer(\n",
    "    model=model_style,\n",
    "    args=args_style,\n",
    "    train_dataset=tokenized_style[\"train\"],\n",
    "    eval_dataset=tokenized_style[\"validation\"], # Note: In real research, ensure validation doesn't overlap\n",
    "    data_collator=DataCollatorForSeq2Seq(tokenizer, model=model_style),\n",
    ")\n",
    "trainer_style.train()\n",
    "model_style.save_pretrained(OUTPUT_DIR_STYLE)\n",
    "tokenizer.save_pretrained(OUTPUT_DIR_STYLE) # Save tokenizer with new tokens\n",
    "\n",
    "print(\"Training Complete!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLP_BART_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
