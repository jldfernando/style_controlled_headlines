{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ed01a415",
   "metadata": {},
   "source": [
    "---\n",
    "title: Pegasus Training Loop\n",
    "author: Josh Fernando\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8c73768",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e933ae56",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/louideval/miniconda3/envs/NLP_BART_env/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU: NVIDIA GeForce RTX 4070 Laptop GPU\n",
      "CUDA Version: 12.8\n",
      "PyTorch Version: 2.9.1+cu128\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, DataCollatorForSeq2Seq, Seq2SeqTrainingArguments, Seq2SeqTrainer, set_seed\n",
    "from peft import get_peft_model, LoraConfig, TaskType\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "print(f\"CUDA Version: {torch.version.cuda}\")\n",
    "print(f\"PyTorch Version: {torch.__version__}\")\n",
    "\n",
    "# --- CONFIGURATION ---\n",
    "SEED = 42 \n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "torch.use_deterministic_algorithms(True, warn_only=True)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)\n",
    "set_seed(SEED) # <-- Set global seed here!\n",
    "\n",
    "\n",
    "MODEL_ID = \"google/pegasus-large\"\n",
    "MAX_INPUT_LEN = 256\n",
    "MAX_TARGET_LEN = 64\n",
    "LR = 2e-4\n",
    "BATCH_SIZE = 8 # Adjust based on GPU VRAM\n",
    "EPOCHS = 5\n",
    "OUTPUT_DIR_BASE = \"../output/baseline_model_PEGASUS\"\n",
    "# OUTPUT_DIR_STYLE = \"../output/style_model_PEGASUS\"\n",
    "\n",
    "# 1. Load Data\n",
    "data_files = {\n",
    "    \"train\": \"../data/processed/train.csv\",\n",
    "    \"validation\": \"../data/processed/dev.csv\"\n",
    "}\n",
    "dataset = load_dataset(\"csv\", data_files=data_files)\n",
    "\n",
    "# 2. Tokenizer Setup & Special Tokens\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID)\n",
    "special_tokens = [\"<neutral>\", \"<punchy>\"]\n",
    "tokenizer.add_tokens(special_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21b763d8",
   "metadata": {},
   "source": [
    "## Preprocessing & LoRA Setup Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fd06bbd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- PREPROCESSING FUNCTIONS ---\n",
    "\n",
    "def preprocess_baseline(examples):\n",
    "    # Baseline: Input = Snippet, Target = Neutral Headline (or random)\n",
    "    inputs = examples['snippet']\n",
    "    targets = examples['headline'] # Using actual headline as the default target for baseline\n",
    "    \n",
    "    model_inputs = tokenizer(inputs, max_length=MAX_INPUT_LEN, truncation=True)\n",
    "    labels = tokenizer(targets, max_length=MAX_TARGET_LEN, truncation=True)\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "def preprocess_style(examples):\n",
    "    # Style Model: Input = <STYLE> + Snippet, Target = Specific Headline\n",
    "    inputs = []\n",
    "    targets = []\n",
    "    \n",
    "    styles = ['neutral', 'punchy']\n",
    "    style_tokens = {'neutral': '<neutral>', 'punchy': '<punchy>'}\n",
    "    \n",
    "    for i in range(len(examples['snippet'])):\n",
    "        for style in styles:\n",
    "            # Construct Input: \"<style> snippet\"\n",
    "            input_text = f\"{style_tokens[style]} {examples['snippet'][i]}\"\n",
    "            # Construct Target: corresponding headline\n",
    "            target_text = examples[f'{style}'][i]\n",
    "            \n",
    "            inputs.append(input_text)\n",
    "            targets.append(target_text)\n",
    "            \n",
    "    model_inputs = tokenizer(inputs, max_length=MAX_INPUT_LEN, truncation=True)\n",
    "    labels = tokenizer(targets, max_length=MAX_TARGET_LEN, truncation=True)\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "# 3. Create Processed Datasets\n",
    "tokenized_baseline = dataset.map(preprocess_baseline, batched=True)\n",
    "# Note: Style preprocessing expands dataset size (1 story -> 3 pairs), so we map carefully\n",
    "# For simplicity in this script, we map then flatten, or use a custom generator. \n",
    "# Here is a simplified approach re-loading for style to ensure 1-to-many mapping:\n",
    "def flatten_style_data(batch):\n",
    "    new_rows = {'input': [], 'target': []}\n",
    "    styles = {'neutral': '<neutral>', 'punchy': '<punchy>'}\n",
    "    for i, snippet in enumerate(batch['snippet']):\n",
    "        for style_name, token in styles.items():\n",
    "            new_rows['input'].append(f\"{token} {snippet}\")\n",
    "            new_rows['target'].append(batch[f'{style_name}'][i])\n",
    "    return new_rows\n",
    "\n",
    "style_dataset = dataset.map(flatten_style_data, batched=True, remove_columns=dataset['train'].column_names)\n",
    "tokenized_style = style_dataset.map(lambda x: {\n",
    "    'input_ids': tokenizer(x['input'], max_length=MAX_INPUT_LEN, truncation=True)['input_ids'],\n",
    "    'labels': tokenizer(x['target'], max_length=MAX_TARGET_LEN, truncation=True)['input_ids']\n",
    "}, batched=True)\n",
    "\n",
    "# 4. LoRA Setup Function\n",
    "def get_lora_model(rank=8, target_modules=['q_proj', 'v_proj']):\n",
    "    model = AutoModelForSeq2SeqLM.from_pretrained(MODEL_ID)\n",
    "    model.resize_token_embeddings(len(tokenizer)) # Resize for new style tokens\n",
    "    \n",
    "    peft_config = LoraConfig(\n",
    "        task_type=TaskType.SEQ_2_SEQ_LM, \n",
    "        inference_mode=False, \n",
    "        r=rank,\n",
    "        lora_alpha=2*rank, \n",
    "        lora_dropout=0.1,\n",
    "        target_modules=target_modules,)\n",
    "    model = get_peft_model(model, peft_config)\n",
    "    model.print_trainable_parameters()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b1fdd3b",
   "metadata": {},
   "source": [
    "## Train Baseline Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b79b66f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Training Baseline Model ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of PegasusForConditionalGeneration were not initialized from the model checkpoint at google/pegasus-large and are newly initialized: ['model.decoder.embed_positions.weight', 'model.encoder.embed_positions.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 1,572,864 || all params: 572,371,968 || trainable%: 0.2748\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='185' max='185' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [185/185 00:55, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>4.378600</td>\n",
       "      <td>3.476131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>3.091300</td>\n",
       "      <td>2.685733</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>2.760300</td>\n",
       "      <td>2.471124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>2.838300</td>\n",
       "      <td>2.410931</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>2.651700</td>\n",
       "      <td>2.392241</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 5. Train Baseline\n",
    "print(\"\\n--- Training Baseline Model ---\")\n",
    "model_baseline = get_lora_model()\n",
    "args_base = Seq2SeqTrainingArguments(\n",
    "    output_dir=OUTPUT_DIR_BASE,\n",
    "    learning_rate=LR,\n",
    "    num_train_epochs=EPOCHS,\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_steps=10,\n",
    "    seed=SEED\n",
    ")\n",
    "\n",
    "trainer_base = Seq2SeqTrainer(\n",
    "    model=model_baseline,\n",
    "    args=args_base,\n",
    "    train_dataset=tokenized_baseline[\"train\"],\n",
    "    eval_dataset=tokenized_baseline[\"validation\"],\n",
    "    data_collator=DataCollatorForSeq2Seq(tokenizer, model=model_baseline),\n",
    ")\n",
    "trainer_base.train()\n",
    "model_baseline.save_pretrained(OUTPUT_DIR_BASE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bb09a3e",
   "metadata": {},
   "source": [
    "## Train Style Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "89737a73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Training Style-Controlled Models ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of PegasusForConditionalGeneration were not initialized from the model checkpoint at google/pegasus-large and are newly initialized: ['model.decoder.embed_positions.weight', 'model.encoder.embed_positions.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 1,572,864 || all params: 572,371,968 || trainable%: 0.2748\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='370' max='370' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [370/370 01:36, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>3.776300</td>\n",
       "      <td>3.073614</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>3.145800</td>\n",
       "      <td>2.642569</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>2.901600</td>\n",
       "      <td>2.507086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>2.817100</td>\n",
       "      <td>2.457183</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>2.900200</td>\n",
       "      <td>2.442826</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of PegasusForConditionalGeneration were not initialized from the model checkpoint at google/pegasus-large and are newly initialized: ['model.decoder.embed_positions.weight', 'model.encoder.embed_positions.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 3,145,728 || all params: 573,944,832 || trainable%: 0.5481\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='370' max='370' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [370/370 01:27, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>3.266300</td>\n",
       "      <td>2.805567</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2.938700</td>\n",
       "      <td>2.491304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>2.656300</td>\n",
       "      <td>2.398459</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>2.588600</td>\n",
       "      <td>2.356173</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>2.754500</td>\n",
       "      <td>2.349295</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of PegasusForConditionalGeneration were not initialized from the model checkpoint at google/pegasus-large and are newly initialized: ['model.decoder.embed_positions.weight', 'model.encoder.embed_positions.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 6,291,456 || all params: 577,090,560 || trainable%: 1.0902\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='370' max='370' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [370/370 01:25, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.986900</td>\n",
       "      <td>2.567326</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2.713700</td>\n",
       "      <td>2.361155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>2.467500</td>\n",
       "      <td>2.284897</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>2.404100</td>\n",
       "      <td>2.250947</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>2.548800</td>\n",
       "      <td>2.245611</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of PegasusForConditionalGeneration were not initialized from the model checkpoint at google/pegasus-large and are newly initialized: ['model.decoder.embed_positions.weight', 'model.encoder.embed_positions.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 12,582,912 || all params: 583,382,016 || trainable%: 2.1569\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='370' max='370' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [370/370 01:30, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.725600</td>\n",
       "      <td>2.428195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2.588900</td>\n",
       "      <td>2.296451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>2.341700</td>\n",
       "      <td>2.227006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>2.263800</td>\n",
       "      <td>2.209273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>2.439600</td>\n",
       "      <td>2.206008</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of PegasusForConditionalGeneration were not initialized from the model checkpoint at google/pegasus-large and are newly initialized: ['model.decoder.embed_positions.weight', 'model.encoder.embed_positions.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 5,767,168 || all params: 576,566,272 || trainable%: 1.0003\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='370' max='370' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [370/370 02:31, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>3.605700</td>\n",
       "      <td>2.696144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2.838500</td>\n",
       "      <td>2.389969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>2.543500</td>\n",
       "      <td>2.298279</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>2.411800</td>\n",
       "      <td>2.258296</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>2.579000</td>\n",
       "      <td>2.251112</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of PegasusForConditionalGeneration were not initialized from the model checkpoint at google/pegasus-large and are newly initialized: ['model.decoder.embed_positions.weight', 'model.encoder.embed_positions.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 11,534,336 || all params: 582,333,440 || trainable%: 1.9807\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='370' max='370' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [370/370 02:59, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>3.189400</td>\n",
       "      <td>2.486214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2.591900</td>\n",
       "      <td>2.280167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>2.310300</td>\n",
       "      <td>2.204924</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>2.220800</td>\n",
       "      <td>2.185128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>2.368900</td>\n",
       "      <td>2.181582</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of PegasusForConditionalGeneration were not initialized from the model checkpoint at google/pegasus-large and are newly initialized: ['model.decoder.embed_positions.weight', 'model.encoder.embed_positions.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 23,068,672 || all params: 593,867,776 || trainable%: 3.8845\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='370' max='370' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [370/370 03:18, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.686700</td>\n",
       "      <td>2.378242</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2.446300</td>\n",
       "      <td>2.207739</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>2.117800</td>\n",
       "      <td>2.161018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>2.061500</td>\n",
       "      <td>2.145142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>2.163400</td>\n",
       "      <td>2.148400</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of PegasusForConditionalGeneration were not initialized from the model checkpoint at google/pegasus-large and are newly initialized: ['model.decoder.embed_positions.weight', 'model.encoder.embed_positions.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 46,137,344 || all params: 616,936,448 || trainable%: 7.4785\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='370' max='370' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [370/370 03:57, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.598700</td>\n",
       "      <td>2.307817</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2.331000</td>\n",
       "      <td>2.181708</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>2.053300</td>\n",
       "      <td>2.144907</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.956300</td>\n",
       "      <td>2.138778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>2.013200</td>\n",
       "      <td>2.152333</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Complete!\n"
     ]
    }
   ],
   "source": [
    "style_configs = [\n",
    "    {\"directory\": \"../output/Pegasus/style_model_PEGASUS_r8A\", \"rank\": 8, \"target_modules\": ['q_proj', 'v_proj']},\n",
    "    {\"directory\": \"../output/Pegasus/style_model_PEGASUS_r16A\", \"rank\": 16, \"target_modules\": ['q_proj', 'v_proj']},\n",
    "    {\"directory\": \"../output/Pegasus/style_model_PEGASUS_r32A\", \"rank\": 32, \"target_modules\": [\"q_proj\", \"v_proj\"]},\n",
    "    {\"directory\": \"../output/Pegasus/style_model_PEGASUS_r64A\", \"rank\": 64, \"target_modules\": [\"q_proj\", \"v_proj\"]},\n",
    "    \n",
    "    {\"directory\": \"../output/Pegasus/style_model_PEGASUS_r8AFFN\", \"rank\": 8, \"target_modules\": [\"q_proj\", \"v_proj\", \"k_proj\", \"out_proj\", \"fc1\", \"fc2\"]},\n",
    "    {\"directory\": \"../output/Pegasus/style_model_PEGASUS_r16AFFN\", \"rank\": 16, \"target_modules\": [\"q_proj\", \"v_proj\", \"k_proj\", \"out_proj\", \"fc1\", \"fc2\"]},\n",
    "    {\"directory\": \"../output/Pegasus/style_model_PEGASUS_r32AFFN\", \"rank\": 32, \"target_modules\": [\"q_proj\", \"v_proj\", \"k_proj\", \"out_proj\", \"fc1\", \"fc2\"]},\n",
    "    {\"directory\": \"../output/Pegasus/style_model_PEGASUS_r64AFFN\", \"rank\": 64, \"target_modules\": [\"q_proj\", \"v_proj\", \"k_proj\", \"out_proj\", \"fc1\", \"fc2\"]},\n",
    "]\n",
    "\n",
    "# 6. Train Style Models\n",
    "print(\"\\n--- Training Style-Controlled Models ---\")\n",
    "for config in style_configs:\n",
    "    torch.cuda.empty_cache()\n",
    "    set_seed(SEED)\n",
    "    model_style = get_lora_model(rank=config[\"rank\"], target_modules=config[\"target_modules\"]) # Re-initialize fresh model\n",
    "    args_style = Seq2SeqTrainingArguments(\n",
    "        output_dir=config[\"directory\"],\n",
    "        learning_rate=LR,\n",
    "        num_train_epochs=EPOCHS,\n",
    "        per_device_train_batch_size=BATCH_SIZE,\n",
    "        eval_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        logging_steps=10,\n",
    "        seed=SEED,\n",
    "        data_seed=SEED,\n",
    "    )\n",
    "\n",
    "    trainer_style = Seq2SeqTrainer(\n",
    "        model=model_style,\n",
    "        args=args_style,\n",
    "        train_dataset=tokenized_style[\"train\"],\n",
    "        eval_dataset=tokenized_style[\"validation\"],\n",
    "        data_collator=DataCollatorForSeq2Seq(tokenizer, model=model_style),\n",
    "    )\n",
    "    trainer_style.train()\n",
    "    model_style.save_pretrained(config[\"directory\"])\n",
    "    tokenizer.save_pretrained(config[\"directory\"]) # Save tokenizer with new tokens\n",
    "\n",
    "print(\"Training Complete!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLP_BART_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
