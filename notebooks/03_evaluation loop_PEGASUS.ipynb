{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2fb1ecb5",
   "metadata": {},
   "source": [
    "---\n",
    "title: Pegasus Evaluation Loop\n",
    "author: \"Josh Fernando\"\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2474bf5f",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0dfe7d3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/louideval/miniconda3/envs/NLP_BART_env/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import evaluate\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, pipeline\n",
    "from peft import PeftModel, PeftConfig\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Setup\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "BASE_MODEL_ID = \"google/pegasus-large\"\n",
    "\n",
    "# Load Test Data\n",
    "test_df = pd.read_csv('../data/processed/test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a5928e6",
   "metadata": {},
   "source": [
    "## Generate Headlines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a03a6500",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating headlines...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of PegasusForConditionalGeneration were not initialized from the model checkpoint at google/pegasus-large and are newly initialized: ['model.decoder.embed_positions.weight', 'model.encoder.embed_positions.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`\n",
      "100%|██████████| 64/64 [02:28<00:00,  2.32s/it]\n",
      "Some weights of PegasusForConditionalGeneration were not initialized from the model checkpoint at google/pegasus-large and are newly initialized: ['model.decoder.embed_positions.weight', 'model.encoder.embed_positions.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "100%|██████████| 64/64 [02:12<00:00,  2.06s/it]\n",
      "Some weights of PegasusForConditionalGeneration were not initialized from the model checkpoint at google/pegasus-large and are newly initialized: ['model.decoder.embed_positions.weight', 'model.encoder.embed_positions.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "100%|██████████| 64/64 [02:00<00:00,  1.89s/it]\n",
      "Some weights of PegasusForConditionalGeneration were not initialized from the model checkpoint at google/pegasus-large and are newly initialized: ['model.decoder.embed_positions.weight', 'model.encoder.embed_positions.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "100%|██████████| 64/64 [01:45<00:00,  1.64s/it]\n",
      "Some weights of PegasusForConditionalGeneration were not initialized from the model checkpoint at google/pegasus-large and are newly initialized: ['model.decoder.embed_positions.weight', 'model.encoder.embed_positions.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "100%|██████████| 64/64 [02:55<00:00,  2.74s/it]\n",
      "Some weights of PegasusForConditionalGeneration were not initialized from the model checkpoint at google/pegasus-large and are newly initialized: ['model.decoder.embed_positions.weight', 'model.encoder.embed_positions.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "100%|██████████| 64/64 [02:47<00:00,  2.62s/it]\n",
      "Some weights of PegasusForConditionalGeneration were not initialized from the model checkpoint at google/pegasus-large and are newly initialized: ['model.decoder.embed_positions.weight', 'model.encoder.embed_positions.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "100%|██████████| 64/64 [02:24<00:00,  2.26s/it]\n",
      "Some weights of PegasusForConditionalGeneration were not initialized from the model checkpoint at google/pegasus-large and are newly initialized: ['model.decoder.embed_positions.weight', 'model.encoder.embed_positions.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "100%|██████████| 64/64 [02:20<00:00,  2.20s/it]\n"
     ]
    }
   ],
   "source": [
    "models = [\n",
    "    {\"name\": \"LoRA-8 Attention\", \"path\": \"../output/Pegasus/style_model_PEGASUS_r8A\"},\n",
    "    {\"name\": \"LoRA-16 Attention\", \"path\": \"../output/Pegasus/style_model_PEGASUS_r16A\"},\n",
    "    {\"name\": \"LoRA-32 Attention\", \"path\": \"../output/Pegasus/style_model_PEGASUS_r32A\"},\n",
    "    {\"name\": \"LoRA-64 Attention\", \"path\": \"../output/Pegasus/style_model_PEGASUS_r64A\"},\n",
    "    {\"name\": \"LoRA-8 Attention+FFN\", \"path\": \"../output/Pegasus/style_model_PEGASUS_r8AFFN\"},\n",
    "    {\"name\": \"LoRA-16 Attention+FFN\", \"path\": \"../output/Pegasus/style_model_PEGASUS_r16AFFN\"},\n",
    "    {\"name\": \"LoRA-32 Attention+FFN\", \"path\": \"../output/Pegasus/style_model_PEGASUS_r32AFFN\"},\n",
    "    {\"name\": \"LoRA-64 Attention+FFN\", \"path\": \"../output/Pegasus/style_model_PEGASUS_r64AFFN\"},\n",
    "]\n",
    "\n",
    "results_pred = dict()\n",
    "\n",
    "print(\"Generating headlines...\")\n",
    "for pegasus in models:\n",
    "    # Setup\n",
    "    STYLE_ADAPTER_PATH = pegasus[\"path\"]\n",
    "    tokenizer = AutoTokenizer.from_pretrained(STYLE_ADAPTER_PATH)\n",
    "    base_model = AutoModelForSeq2SeqLM.from_pretrained(BASE_MODEL_ID)\n",
    "    base_model.resize_token_embeddings(len(tokenizer))\n",
    "    model = PeftModel.from_pretrained(base_model, STYLE_ADAPTER_PATH)\n",
    "    model.to(DEVICE)\n",
    "    model.eval()\n",
    "\n",
    "    # Generation Loop\n",
    "    results = []\n",
    "    styles = ['neutral', 'punchy']\n",
    "    style_tokens = {'neutral': '<neutral>', 'punchy': '<punchy>'}\n",
    "\n",
    "    for idx, row in tqdm(test_df.iterrows(), total=len(test_df)):\n",
    "        snippet = row['snippet']\n",
    "        \n",
    "        entry = {'snippet': snippet, 'ref_neutral': row['neutral'], \n",
    "                'ref_punchy': row['punchy']}\n",
    "        \n",
    "        # --- 1. Style-Controlled Model Generation ---\n",
    "        for style in styles:\n",
    "            input_text = f\"{style_tokens[style]} {snippet}\"\n",
    "            inputs = tokenizer(input_text, return_tensors=\"pt\").to(DEVICE)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                if style == 'punchy':\n",
    "                    outputs = model.generate(\n",
    "                        **inputs,\n",
    "                        max_new_tokens=64,\n",
    "                        do_sample=True,             # Use sampling\n",
    "                        top_p=0.9,                  # Nucleus sampling\n",
    "                        temperature=0.8,            # High temperature for creativity\n",
    "                        num_return_sequences=1      # Ensure you only get one output\n",
    "                    )\n",
    "                else: # neutral style, stick to high-fidelity generation\n",
    "                    outputs = model.generate(\n",
    "                        **inputs, \n",
    "                        max_new_tokens=64,\n",
    "                        num_beams=4,\n",
    "                    )\n",
    "            \n",
    "            gen_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "            entry[f'gen_{style}'] = gen_text\n",
    "            \n",
    "        # --- 2. Baseline Model Generation ---\n",
    "        # Baseline uses just the snippet as input, no style token\n",
    "        baseline_inputs = tokenizer(snippet, return_tensors=\"pt\").to(DEVICE)\n",
    "        with torch.no_grad():\n",
    "            baseline_outputs = base_model.generate(**baseline_inputs, max_new_tokens=64) # Uses base_model\n",
    "        \n",
    "        baseline_text = tokenizer.decode(baseline_outputs[0], skip_special_tokens=True)\n",
    "        entry['gen_baseline'] = baseline_text\n",
    "        \n",
    "        results.append(entry)\n",
    "\n",
    "    results_df = pd.DataFrame(results)\n",
    "    results_pred.update({pegasus[\"name\"]:results_df})\n",
    "    del results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "64936391",
   "metadata": {},
   "outputs": [],
   "source": [
    "for model_name in results_pred.keys():\n",
    "    results_pred[model_name].to_csv(f\"../data/predictions/Pegasus_{model_name}.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9ac1527",
   "metadata": {},
   "source": [
    "## ROGUE Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6582711e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- ROUGE Scores ---\n"
     ]
    }
   ],
   "source": [
    "# 3. Metric 1: ROUGE Score\n",
    "rouge = evaluate.load('rouge')\n",
    "results_rouge = dict()\n",
    "print(\"\\n--- ROUGE Scores ---\")\n",
    "\n",
    "for model in results_pred.keys():\n",
    "    rouge_scores = pd.DataFrame(columns=[\"rouge_1\", \"rouge_2\", \"rouge_L\", \"rouge_Lsum\"])\n",
    "    results_df = results_pred[model]\n",
    "    # Style-Controlled Model Scores (Existing)\n",
    "    for style in styles:\n",
    "        refs = results_df[f'ref_{style}'].tolist()\n",
    "        preds = results_df[f'gen_{style}'].tolist()\n",
    "        score = rouge.compute(predictions=preds, references=refs)\n",
    "        # print(f\"Style Model <{style.upper()}>: {score}\")\n",
    "        rouge_scores.loc[f\"<{style.upper()}> vs. REF_{style.upper()}\"] = score.values()\n",
    "\n",
    "    # Baseline Model Scores (NEW)\n",
    "    baseline_preds = results_df['gen_baseline'].tolist()\n",
    "\n",
    "    # Compare Baseline against NEUTRAL reference\n",
    "    refs_neutral = results_df['ref_neutral'].tolist()\n",
    "    score_baseline_neutral = rouge.compute(predictions=baseline_preds, references=refs_neutral)\n",
    "    # print(f\"Baseline Model vs. REF_NEUTRAL: {score_baseline_neutral}\")\n",
    "    rouge_scores.loc[f\"Baseline Model vs. REF_NEUTRAL\"] = score_baseline_neutral.values()\n",
    "\n",
    "    # Compare Baseline against PUNCHY reference\n",
    "    refs_punchy = results_df['ref_punchy'].tolist()\n",
    "    score_baseline_punchy = rouge.compute(predictions=baseline_preds, references=refs_punchy)\n",
    "    # print(f\"Baseline Model vs. REF_PUNCHY: {score_baseline_punchy}\")\n",
    "    rouge_scores.loc[f\"Baseline Model vs. REF_PUNCHY\"] = score_baseline_punchy.values()\n",
    "\n",
    "    results_rouge.update({model:rouge_scores})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6b5e1f51",
   "metadata": {},
   "outputs": [],
   "source": [
    "for model_name in results_rouge.keys():\n",
    "    results_rouge[model_name].to_csv(f\"../data/rouge/Pegasus_{model_name}.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0a9bfbe",
   "metadata": {},
   "source": [
    "## Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5e37174f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Training Proxy Style Classifier ---\n",
      "Proxy Style Classifier Accuracy (on human data): 59.32%\n",
      "\n",
      "--- Evaluating Style Accuracy ---\n"
     ]
    }
   ],
   "source": [
    "# 4. Metric 2: Style Accuracy (Simple Proxy Classifier)\n",
    "# We train a quick classifier on the TRAIN set to act as our evaluator\n",
    "print(\"\\n--- Training Proxy Style Classifier ---\")\n",
    "train_df = pd.read_csv('../data/processed/train.csv')\n",
    "train_texts = []\n",
    "train_labels = []\n",
    "\n",
    "# Unpack training data for classifier\n",
    "for _, row in train_df.iterrows():\n",
    "    train_texts.extend([row['neutral'], row['punchy']])\n",
    "    train_labels.extend(['neutral', 'punchy'])\n",
    "\n",
    "# 1. Split the human data for Classifier Training/Testing\n",
    "X_classifier_train, X_classifier_test, y_classifier_train, y_classifier_test = train_test_split(\n",
    "    train_texts, train_labels, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "vectorizer = TfidfVectorizer(ngram_range=(1,5))\n",
    "X_train_vec = vectorizer.fit_transform(X_classifier_train)\n",
    "X_test_vec = vectorizer.transform(X_classifier_test)\n",
    "\n",
    "clf = LogisticRegression(max_iter=1000)\n",
    "clf.fit(X_train_vec, y_classifier_train)\n",
    "\n",
    "# 2. Calculate Classifier Accuracy on its own test set\n",
    "y_pred = clf.predict(X_test_vec)\n",
    "classifier_acc = accuracy_score(y_classifier_test, y_pred)\n",
    "\n",
    "print(f\"Proxy Style Classifier Accuracy (on human data): {classifier_acc:.2%}\")\n",
    "\n",
    "# Evaluate Generated Headlines for All Models\n",
    "results_classification = dict()\n",
    "print(\"\\n--- Evaluating Style Accuracy ---\")\n",
    "\n",
    "for model in results_pred.keys():\n",
    "    classification_scores = pd.DataFrame(columns=[\"Accuracy\", \"Neutral_Pct\", \"Punchy_Pct\"])\n",
    "    results_df = results_pred[model]\n",
    "    \n",
    "    # Style-Controlled Model Evaluation\n",
    "    for style in styles:\n",
    "        gen_texts = results_df[f'gen_{style}'].tolist()\n",
    "        X_test = vectorizer.transform(gen_texts)\n",
    "        preds = clf.predict(X_test)\n",
    "        \n",
    "        # Calculate how many matched the requested style\n",
    "        matches = [1 if p == style else 0 for p in preds]\n",
    "        acc = sum(matches) / len(matches)\n",
    "        \n",
    "        # Calculate distribution\n",
    "        from collections import Counter\n",
    "        style_dist = Counter(preds)\n",
    "        total = len(preds)\n",
    "        neutral_pct = style_dist.get('neutral', 0) / total\n",
    "        punchy_pct = style_dist.get('punchy', 0) / total\n",
    "        \n",
    "        classification_scores.loc[f\"<{style.upper()}>\"] = [acc, neutral_pct, punchy_pct]\n",
    "    \n",
    "    # Baseline Model Evaluation\n",
    "    baseline_gen_texts = results_df['gen_baseline'].tolist()\n",
    "    X_test_baseline = vectorizer.transform(baseline_gen_texts)\n",
    "    preds_baseline = clf.predict(X_test_baseline)\n",
    "    \n",
    "    # Calculate the distribution of predicted styles for the baseline\n",
    "    baseline_style_distribution = Counter(preds_baseline)\n",
    "    total_count = len(preds_baseline)\n",
    "    baseline_neutral_pct = baseline_style_distribution.get('neutral', 0) / total_count\n",
    "    baseline_punchy_pct = baseline_style_distribution.get('punchy', 0) / total_count\n",
    "    \n",
    "    # For baseline, accuracy is not applicable (no target style), so we use None\n",
    "    classification_scores.loc[\"Baseline\"] = [None, baseline_neutral_pct, baseline_punchy_pct]\n",
    "    \n",
    "    results_classification.update({model: classification_scores})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b95031a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "for model_name in results_classification.keys():\n",
    "    results_classification[model_name].to_csv(f\"../data/classification/Pegasus_{model_name}.csv\", index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e380cf9",
   "metadata": {},
   "source": [
    "## Entailment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8417a25b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Evaluating Factuality (NLI) ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-large-mnli were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Evaluating Factuality (NLI) ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluation Complete!\n"
     ]
    }
   ],
   "source": [
    "# 5. Metric 3: Factuality (NLI)\n",
    "# Using a small NLI model to check entailment\n",
    "print(\"\\n--- Evaluating Factuality (NLI) ---\")\n",
    "nli_pipeline = pipeline(\"text-classification\", model=\"roberta-large-mnli\", device=0 if DEVICE==\"cuda\" else -1)\n",
    "\n",
    "def get_entailment_score(premise, hypothesis):\n",
    "    # NLI input format: \"Premise </s></s> Hypothesis\" (model specific, but pipeline handles pairs usually)\n",
    "    # roberta-large-mnli labels: CONTRADICTION, NEUTRAL, ENTAILMENT\n",
    "    # We pass text_pair to pipeline\n",
    "    result = nli_pipeline({'text': premise, 'text_pair': hypothesis})\n",
    "    # We want to know if it is NOT contradiction, or strictly entailment\n",
    "    # Let's track Entailment probability\n",
    "    return result['score'] if result['label'] == 'ENTAILMENT' else 0.0\n",
    "\n",
    "# Evaluate Generated Headlines for All Models\n",
    "results_entailment = dict()\n",
    "print(\"\\n--- Evaluating Factuality (NLI) ---\")\n",
    "\n",
    "for model in results_pred.keys():\n",
    "    entailment_scores = pd.DataFrame(columns=[\"Avg_Entailment_Score\"])\n",
    "    results_df = results_pred[model]\n",
    "    \n",
    "    # Style-Controlled Model Evaluation\n",
    "    for style in styles:\n",
    "        scores = []\n",
    "        for _, row in results_df.iterrows():\n",
    "            score = get_entailment_score(row['snippet'], row[f'gen_{style}'])\n",
    "            scores.append(score)\n",
    "        avg_fact = sum(scores)/len(scores)\n",
    "        entailment_scores.loc[f\"<{style.upper()}>\"] = [avg_fact]\n",
    "    \n",
    "    # Baseline Model Evaluation\n",
    "    baseline_scores = []\n",
    "    for _, row in results_df.iterrows():\n",
    "        score = get_entailment_score(row['snippet'], row['gen_baseline'])\n",
    "        baseline_scores.append(score)\n",
    "    avg_fact_baseline = sum(baseline_scores)/len(baseline_scores)\n",
    "    entailment_scores.loc[\"Baseline\"] = [avg_fact_baseline]\n",
    "    \n",
    "    results_entailment.update({model: entailment_scores})\n",
    "\n",
    "print(\"\\nEvaluation Complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "00f24dfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "for model_name in results_entailment.keys():\n",
    "    results_entailment[model_name].to_csv(f\"../data/entailment/Pegasus_{model_name}.csv\", index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6e08c97",
   "metadata": {},
   "source": [
    "## Identical Outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9876f401",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Evaluating Identical Outputs ---\n",
      "LoRA-8 Attention: 41/64 (64.06%)\n",
      "LoRA-16 Attention: 44/64 (68.75%)\n",
      "LoRA-32 Attention: 39/64 (60.94%)\n",
      "LoRA-64 Attention: 46/64 (71.88%)\n",
      "LoRA-8 Attention+FFN: 41/64 (64.06%)\n",
      "LoRA-16 Attention+FFN: 42/64 (65.62%)\n",
      "LoRA-32 Attention+FFN: 48/64 (75.00%)\n",
      "LoRA-64 Attention+FFN: 47/64 (73.44%)\n",
      "\n",
      "Identical Outputs Evaluation Complete!\n"
     ]
    }
   ],
   "source": [
    "# 6. Metric 4: Identical Outputs\n",
    "# Count how many times gen_neutral and gen_punchy are identical for each model\n",
    "print(\"\\n--- Evaluating Identical Outputs ---\")\n",
    "\n",
    "results_identical = dict()\n",
    "\n",
    "for model in results_pred.keys():\n",
    "    results_df = results_pred[model]\n",
    "    \n",
    "    # Count identical outputs\n",
    "    identical_count = sum(results_df[\"gen_neutral\"] == results_df[\"gen_punchy\"])\n",
    "    total_count = len(results_df)\n",
    "    identical_percentage = (identical_count / total_count) * 100\n",
    "    \n",
    "    # Store results\n",
    "    identical_scores = pd.DataFrame({\n",
    "        \"Count\": [identical_count],\n",
    "        \"Total\": [total_count],\n",
    "        \"Percentage\": [identical_percentage]\n",
    "    }, index=[\"Identical Outputs\"])\n",
    "    \n",
    "    results_identical.update({model: identical_scores})\n",
    "    \n",
    "    print(f\"{model}: {identical_count}/{total_count} ({identical_percentage:.2f}%)\")\n",
    "\n",
    "print(\"\\nIdentical Outputs Evaluation Complete!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f3b0b69b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for model_name in results_identical.keys():\n",
    "    results_identical[model_name].to_csv(f\"../data/identical_outputs/Pegasus_{model_name}.csv\", index=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLP_BART_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
